{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "Importing Required Libraries"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "import warnings\n",
    "import lightgbm as lgb\n",
    "from datetime import datetime\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_curve, auc\n",
    "from tsfresh import extract_features\n",
    "from tsfresh.feature_extraction.settings import EfficientFCParameters\n",
    "\n",
    "_RANDOM_SEED = 1"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-28T09:16:44.168275900Z",
     "start_time": "2024-07-28T09:16:42.291312400Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Reading the DataSet"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "directory_activity = \"archive\\\\activity_data\"\n",
    "directory_hrv = \"archive\\\\hrv_data\"\n",
    "\n",
    "\n",
    "patient_info = pd.read_csv(\"archive\\\\patient_info.csv\", sep=';')\n",
    "cpt_II = pd.read_csv(\"archive\\\\CPT_II_ConnersContinuousPerformanceTest.csv\", sep=';')\n",
    "patient_info['ID'] = patient_info['ID'].astype(str)\n",
    "cpt_II['ID'] = cpt_II['ID'].astype(str)\n",
    "patient_info.set_index('ID', inplace=True)\n",
    "cpt_II.set_index('ID', inplace=True)\n",
    "\n",
    "hyperaktiv = patient_info.join(cpt_II, how='left')\n",
    "hyperaktiv.dropna(subset=['ACC_TIME', 'HRV_TIME'], inplace=True, how='all')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-28T09:16:44.194120500Z",
     "start_time": "2024-07-28T09:16:44.170269900Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Splitting"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "temp = hyperaktiv.copy()\n",
    "y = temp['ADHD']\n",
    "temp.drop(columns=['ADHD'], inplace=True)\n",
    "x = temp.copy()\n",
    "\n",
    "# x = x.values\n",
    "# y = y.values\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, stratify=y, random_state=_RANDOM_SEED)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-28T09:16:44.201622200Z",
     "start_time": "2024-07-28T09:16:44.195117100Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Feature Engineering"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "def read_activity_file(filepath, patient_id):\n",
    "    data = []\n",
    "    with open(filepath) as f:\n",
    "        csv_reader = csv.reader(f, delimiter=\";\")\n",
    "        next(csv_reader)\n",
    "        for line in csv_reader:\n",
    "            data.append([datetime.strptime(line[0], \"%m-%d-%Y %H:%M\").timestamp(),\n",
    "                         int(line[1].split(\" \")[0])])\n",
    "    data = pd.DataFrame(data, columns=[\"TIME\", \"ACC\"])\n",
    "    data[\"ID\"] = patient_id\n",
    "    return data\n",
    "\n",
    "def read_hrv_file(filepath, patient_id):\n",
    "    data = []\n",
    "    with open(filepath) as f:\n",
    "        csv_reader = csv.reader(f, delimiter=\";\")\n",
    "        next(csv_reader)\n",
    "        for line in csv_reader:\n",
    "            data.append([datetime.strptime(line[0], \"%Y-%m-%d %H:%M:%S.%f\").timestamp(),\n",
    "                         float(line[1].split(\" \")[0])])\n",
    "\n",
    "    data = pd.DataFrame(data, columns=[\"TIME\", \"HRV\"])\n",
    "    data[\"ID\"] = patient_id\n",
    "    return data\n",
    "\n",
    "def create_feature_ACC(row):\n",
    "    patient_id = int(row.name)\n",
    "    print(\"activity number \", patient_id)\n",
    "    filepath = f\"archive\\\\activity_data\\\\patient_activity_{patient_id:02d}.csv\"\n",
    "\n",
    "    if not os.path.exists(filepath):\n",
    "        filepath = \"archive\\\\activity_data\\\\patient_activity_26.csv\"\n",
    "        activity_data = read_activity_file(filepath, patient_id)\n",
    "        print(f\"patient_activity_{patient_id}.csv is missing\")\n",
    "        features = extract_features(activity_data, column_id=\"ID\", column_value=\"ACC\", column_sort=\"TIME\",\n",
    "                                n_jobs=0, show_warnings=False,\n",
    "                                default_fc_parameters=EfficientFCParameters())\n",
    "        # print(f\"inserted NaN instead of patient_activity_{patient_id}.csv\")\n",
    "        features[:] = np.nan\n",
    "        return features\n",
    "\n",
    "    activity_data = read_activity_file(filepath, patient_id)\n",
    "    features = extract_features(activity_data, column_id=\"ID\", column_value=\"ACC\", column_sort=\"TIME\",\n",
    "                                n_jobs=0, show_warnings=False,\n",
    "                                default_fc_parameters=EfficientFCParameters())\n",
    "    # features.to_csv(feature_filepath, index=False, sep=\";\")\n",
    "    return features\n",
    "\n",
    "def create_feature_HRV(row):\n",
    "    patient_id = int(row.name)\n",
    "    print(\"hrv number \", patient_id)\n",
    "    filepath = f\"archive\\\\hrv_data\\\\patient_hr_{patient_id}.csv\"\n",
    "\n",
    "    if not os.path.exists(filepath):\n",
    "        filepath = \"archive\\\\hrv_data\\\\patient_hr_24.csv\"\n",
    "        hrv_data = read_hrv_file(filepath, patient_id)\n",
    "\n",
    "        print(f\"patient_hr_{patient_id}.csv is missing\")\n",
    "        features = extract_features(hrv_data, column_id=\"ID\", column_value=\"HRV\", column_sort=\"TIME\",\n",
    "                                n_jobs=0, show_warnings=False,\n",
    "                                default_fc_parameters=EfficientFCParameters())\n",
    "        # print(f\"inserted NaN instead of patient_hr_{patient_id}.csv\")\n",
    "\n",
    "        features[:] = np.nan\n",
    "        return features\n",
    "\n",
    "    hrv_data = read_hrv_file(filepath, patient_id)\n",
    "    features = extract_features(hrv_data, column_id=\"ID\", column_value=\"HRV\", column_sort=\"TIME\",\n",
    "                                n_jobs=0, show_warnings=False,\n",
    "                                default_fc_parameters=EfficientFCParameters())\n",
    "    # features.to_csv(feature_filepath, index=False, sep=\";\")\n",
    "    return features\n",
    "\n",
    "def create_feature(row):\n",
    "    features_ACC = create_feature_ACC(row)\n",
    "    features_HRV = create_feature_HRV(row)\n",
    "\n",
    "    features = pd.concat([features_ACC, features_HRV], axis=1)\n",
    "    # features = features_ACC\n",
    "    for feature_name in features.columns:\n",
    "        row[feature_name] = features[feature_name].values[0]\n",
    "    return row\n",
    "\n",
    "\n",
    "\n",
    "directory_train = f\"archive_edited\\\\seed_{_RANDOM_SEED}\\\\x_train\"\n",
    "directory_test = f\"archive_edited\\\\seed_{_RANDOM_SEED}\\\\x_test\"\n",
    "\n",
    "if not (os.path.exists(directory_train) and os.path.exists(directory_test)):\n",
    "    # if not os.path.exists(directory_train):\n",
    "    #   os.makedirs(directory_train)\n",
    "\n",
    "    # if not os.path.exists(directory_test):\n",
    "    #   os.makedirs(directory_test)\n",
    "\n",
    "    x_train = x_train.apply(create_feature, axis=1)\n",
    "    x_test = x_test.apply(create_feature, axis=1)\n",
    "\n",
    "    x_train.to_csv(os.path.join(directory_train, \"x_train.csv\"))\n",
    "    x_test.to_csv(os.path.join(directory_test, \"x_test.csv\"))\n",
    "else:\n",
    "    x_train = pd.read_csv(os.path.join(directory_train, \"x_train.csv\"))\n",
    "    x_test  = pd.read_csv(os.path.join(directory_test, \"x_test.csv\"))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-28T09:16:44.364896500Z",
     "start_time": "2024-07-28T09:16:44.201622200Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "PreProcessing"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "features_to_drop = [\"ACC__friedrich_coefficients__coeff_0__m_3__r_30\",\n",
    "                    \"ACC__friedrich_coefficients__coeff_1__m_3__r_30\",\n",
    "                    \"ACC__friedrich_coefficients__coeff_2__m_3__r_30\",\n",
    "                    \"ACC__friedrich_coefficients__coeff_3__m_3__r_30\",\n",
    "                    \"ACC__max_langevin_fixed_point__m_3__r_30\",\n",
    "                    \"ACC__query_similarity_count__query_None__threshold_0.0\",\n",
    "                    \"HRV__friedrich_coefficients__coeff_0__m_3__r_30\",\n",
    "                    \"HRV__friedrich_coefficients__coeff_1__m_3__r_30\",\n",
    "                    \"HRV__friedrich_coefficients__coeff_2__m_3__r_30\",\n",
    "                    \"HRV__friedrich_coefficients__coeff_3__m_3__r_30\",\n",
    "                    \"HRV__max_langevin_fixed_point__m_3__r_30\",\n",
    "                    \"HRV__query_similarity_count__query_None__threshold_0.0\",\n",
    "                    'ACC_TIME',\n",
    "                    'HRV_TIME',\n",
    "                    'CPT_II', ]\n",
    "x_train.drop(columns=features_to_drop, inplace=True)\n",
    "x_test.drop(columns=features_to_drop, inplace=True)\n",
    "prep_pipeLine = Pipeline([\n",
    "    (\"imputing\", KNNImputer(n_neighbors=40)),\n",
    "    (\"feature_selecting\", SelectKBest(k=5)),\n",
    "    (\"scaling\", MinMaxScaler())\n",
    "])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-28T09:16:44.370209100Z",
     "start_time": "2024-07-28T09:16:44.364896500Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Results"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Accuracy across 10 folds for weight:1.22: 0.94000000\n",
      "Average Precision across 10 folds for weight:1.22: 0.90000000\n",
      "Average Recall across 10 folds for weight:1.22: 0.99000000\n",
      "Average F1-score across 10 folds for weight:1.22: 0.94285714\n",
      "Average ROC-score across 10 folds for weight:1.22: 0.94000000\n",
      "###########################################################################################\n",
      "\n"
     ]
    }
   ],
   "source": [
    "skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=_RANDOM_SEED)\n",
    "for weight in [1.22]:\n",
    "\n",
    "    accuracy_scores = []\n",
    "    precision_scores = []\n",
    "    recall_scores = []\n",
    "    f1_scores = []\n",
    "    roc_scores = []\n",
    "\n",
    "    for train_index, val_index in skf.split(x_train, y_train):\n",
    "        X_train_fold, X_val_fold = x_train.iloc[train_index], x_train.iloc[val_index]\n",
    "        y_train_fold, y_val_fold = y_train.iloc[train_index], y_train.iloc[val_index]\n",
    "\n",
    "        weights = np.where(y_train_fold == 0, 1.0, weight)\n",
    "\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.simplefilter(\"ignore\", category=UserWarning)\n",
    "            warnings.simplefilter(\"ignore\", category=RuntimeWarning)\n",
    "            prep_pipeLine.fit(X_train_fold, y_train_fold)\n",
    "        X_train_fold = prep_pipeLine.transform(X_train_fold)\n",
    "        X_val_fold = prep_pipeLine.transform(X_val_fold)\n",
    "        x_test_fold = prep_pipeLine.transform(x_test)\n",
    "\n",
    "        train_data = lgb.Dataset(X_train_fold, label=y_train_fold, weight=weights)\n",
    "        val_data = lgb.Dataset(X_val_fold, label=y_val_fold, reference=train_data)\n",
    "        test_data = lgb.Dataset(x_test_fold, label=y_test, reference=train_data)\n",
    "\n",
    "        params = {\n",
    "                    'verbose': -3,\n",
    "                    'max_bin': 4096,\n",
    "                    'extra_trees': True,\n",
    "                    'min_data_in_leaf': 6,\n",
    "                    'feature_fraction': 0.6,\n",
    "                    'learning_rate': 0.03,\n",
    "                    'num_leaves': 128,\n",
    "                 }\n",
    "        num_round = 100\n",
    "        lgb_model = lgb.train(params, train_data, num_boost_round=num_round)\n",
    "\n",
    "        y_pred_proba = lgb_model.predict(x_test_fold)\n",
    "        y_pred = [1 if p > 0.5 else 0 for p in y_pred_proba]\n",
    "\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        precision = precision_score(y_test, y_pred)\n",
    "        recall = recall_score(y_test, y_pred)\n",
    "        f1 = f1_score(y_test, y_pred)\n",
    "        fpr, tpr, thresholds = roc_curve(y_test, y_pred)\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "\n",
    "        accuracy_scores.append(accuracy)\n",
    "        precision_scores.append(precision)\n",
    "        recall_scores.append(recall)\n",
    "        f1_scores.append(f1)\n",
    "        roc_scores.append(roc_auc)\n",
    "\n",
    "    average_accuracy = np.mean(accuracy_scores)\n",
    "    average_precision = np.mean(precision_scores)\n",
    "    average_recall = np.mean(recall_scores)\n",
    "    average_f1 = np.mean(f1_scores)\n",
    "    average_roc = np.mean(roc_scores)\n",
    "\n",
    "    print(f'Average Accuracy across 10 folds for weight:{weight}: {average_accuracy:.8f}')\n",
    "    print(f'Average Precision across 10 folds for weight:{weight}: {average_precision:.8f}')\n",
    "    print(f'Average Recall across 10 folds for weight:{weight}: {average_recall:.8f}')\n",
    "    print(f'Average F1-score across 10 folds for weight:{weight}: {average_f1:.8f}')\n",
    "    print(f'Average ROC-score across 10 folds for weight:{weight}: {average_roc:.8f}')\n",
    "    print(\"###########################################################################################\\n\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-28T09:16:54.703880300Z",
     "start_time": "2024-07-28T09:16:44.375193500Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
